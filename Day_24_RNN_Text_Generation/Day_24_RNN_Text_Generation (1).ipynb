{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "  # **Ultimate Guide to RNNs for Text Generation**"
      ],
      "metadata": {
        "id": "jwrOyMK39YhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction**\n",
        "\n",
        "### **Objective**\n",
        "To build a Recurrent Neural Network (RNN) using TensorFlow/Keras for text generation, demonstrating how RNNs handle sequential data to predict and generate text sequences.\n",
        "\n",
        "### **Key Learnings**\n",
        "- How RNNs process sequential data (text).\n",
        "- Building and training an RNN model with LSTM layers.\n",
        "- Techniques for text generation using trained models.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Metadata and Dataset Overview**\n",
        "\n",
        "### **Dataset Used**\n",
        "- **Dataset Name**: Shakespeare's Works (Text Corpus)\n",
        "- **Source**: [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/shakespeare)\n",
        "- **Description**: A text corpus of Shakespeare's plays and poems.\n",
        "\n",
        "### **Acknowledgement**\n",
        "The dataset is publicly available via TensorFlow Datasets and is widely used for educational purposes in NLP.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Loading and Exploring the Dataset**\n"
      ],
      "metadata": {
        "id": "KFC9dFX09dVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Load the Dataset**"
      ],
      "metadata": {
        "id": "2nDazoqs9hiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "metadata": {
        "id": "vFv5VB6a9aUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Shakespeare dataset\n",
        "path_to_file = tf.keras.utils.get_file(\n",
        "    'shakespeare.txt',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        ")\n",
        "\n",
        "# Read the text\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJfwC26U9f2K",
        "outputId": "0bbcc9b6-c90f-470b-edeb-a06be9dee863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- The dataset is downloaded from a URL using TensorFlow utilities.\n",
        "- The text is loaded as a single string, and its length is printed.\n",
        "- **Output**: The text contains ~1.1 million characters.\n"
      ],
      "metadata": {
        "id": "kCDW-m_w9rP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])  # Print the first 500 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Nqt-1PU9f4y",
        "outputId": "03504447-8806-4e42-a6e7-b55fef369283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Data Preprocessing**"
      ],
      "metadata": {
        "id": "27CDJVyO95DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract unique characters and create mappings\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
        "\n",
        "print(f'Unique characters: {len(vocab)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjMrw-yk9f7Y",
        "outputId": "71256e06-856b-4116-edc9-304ca7162278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretation**\n",
        "- The vocabulary consists of **65 unique characters** (letters, punctuation, and symbols).\n",
        "- `char2idx` and `idx2char` are dictionaries for converting characters to integers and vice versa.\n",
        "\n"
      ],
      "metadata": {
        "id": "viPycFB69940"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Convert Text to Numerical Indices**"
      ],
      "metadata": {
        "id": "kHJttqxv-B33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the entire text to numerical indices\n",
        "text_as_int = np.array([char2idx[char] for char in text])\n",
        "print(f'Text sample (numerical): {text_as_int[:10]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXZDWigO9f9y",
        "outputId": "8694dc7f-b254-4016-b576-f839b07dc821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text sample (numerical): [18 47 56 57 58  1 15 47 58 47]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- Each character in the text is replaced with its corresponding integer index.\n"
      ],
      "metadata": {
        "id": "UeQxdtoq-Ld3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Create Training Sequences**"
      ],
      "metadata": {
        "id": "NO0vGHJs-RMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sequence length and batch size\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "# Create training sequences\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "# Split sequences into input and target\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "id": "NaFSfEJP9gAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretation**\n",
        "- **Sequence Length**: 100 characters per sequence.\n",
        "- Each sequence is split into:\n",
        "  - **Input**: First 100 characters.\n",
        "  - **Target**: Next 100 characters (shifted by one character).\n",
        "- The model will learn to predict the next character given the previous sequence.\n"
      ],
      "metadata": {
        "id": "9nnNc3EB-W7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Building the RNN Model**\n",
        "\n",
        "### **Key Concepts**\n",
        "1. **Recurrent Neural Networks (RNNs)**:\n",
        "   - Designed for sequential data.\n",
        "   - Maintain a hidden state that captures information from previous steps.\n",
        "2. **Long Short-Term Memory (LSTM)**:\n",
        "   - A type of RNN that mitigates the vanishing gradient problem.\n",
        "   - Uses gates to control information flow.\n"
      ],
      "metadata": {
        "id": "jNUT1Nx0-eFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Mathematical Intutition**\n",
        "\n",
        "#### **1. Forget Gate**\n",
        "- **Equation**:  \n",
        "  `f_t = sigmoid(W_f * [h_prev, x_t] + b_f)`  \n",
        "- **Explanation**:  \n",
        "  - **`f_t`**: \"Forget gate\" output (values between 0 and 1).  \n",
        "  - **`W_f`**: Weights for the forget gate.  \n",
        "  - **`h_prev`**: Hidden state from the previous timestep.  \n",
        "  - **`x_t`**: Current input.  \n",
        "  - **`b_f`**: Bias term for the forget gate.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Input Gate**\n",
        "- **Equation**:  \n",
        "  `i_t = sigmoid(W_i * [h_prev, x_t] + b_i)`  \n",
        "  `C_tilde_t = tanh(W_C * [h_prev, x_t] + b_C)`  \n",
        "- **Explanation**:  \n",
        "  - **`i_t`**: \"Input gate\" output (values between 0 and 1).  \n",
        "  - **`C_tilde_t`**: Candidate cell state (values between -1 and 1).  \n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Update Cell State**\n",
        "- **Equation**:  \n",
        "  `C_t = f_t * C_prev + i_t * C_tilde_t`  \n",
        "- **Explanation**:  \n",
        "  - **`C_t`**: New cell state (long-term memory).  \n",
        "  - **`C_prev`**: Cell state from the previous timestep.  \n",
        "  - **`*`**: Element-wise multiplication.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Output Gate**\n",
        "- **Equation**:  \n",
        "  `o_t = sigmoid(W_o * [h_prev, x_t] + b_o)`  \n",
        "  `h_t = o_t * tanh(C_t)`  \n",
        "- **Explanation**:  \n",
        "  - **`o_t`**: \"Output gate\" output (values between 0 and 1).  \n",
        "  - **`h_t`**: New hidden state (short-term memory).  \n",
        "\n",
        "---\n",
        "\n",
        "### **What Do These Equations Do?**\n",
        "1. **Forget Gate**: Decides what to remove from long-term memory (`C_prev`).  \n",
        "   - Example: If `f_t` is 0.2 for a piece of information, it retains 20% of it.  \n",
        "2. **Input Gate**: Decides what new information to add to long-term memory.  \n",
        "   - `i_t` acts as a filter, and `C_tilde_t` holds candidate values.  \n",
        "3. **Update Cell State**: Combines old memory (after forgetting) and new memory.  \n",
        "4. **Output Gate**: Decides what part of the updated memory to output as `h_t`.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "- LSTMs use **three gates** (forget, input, output) to manage information flow.  \n",
        "- The **cell state** (`C_t`) acts as long-term memory, while the **hidden state** (`h_t`) is short-term memory.  \n",
        "- **Sigmoid** (0-1) and **tanh** (-1 to 1) functions control how much information is added/removed.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z72295Kq-f9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Define the Model**"
      ],
      "metadata": {
        "id": "GqHNkI_C-hfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "batch_size = 64\n",
        "\n",
        "# Regularized model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    LSTM(rnn_units, return_sequences=True, dropout=0.2),  # Add dropout\n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "f_T2MBPT9gCg",
        "outputId": "a5617f32-ac74-4641-f0d1-e5243c48d08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Training the Model**\n",
        "\n",
        "### **Code: Prepare Batches**\n"
      ],
      "metadata": {
        "id": "AXwsjAbX_Xad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch and shuffle the dataset\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ],
      "metadata": {
        "id": "LcMeq09d-j7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- **Embedding Layer**: Converts character indices into dense vectors.\n",
        "- **LSTM Layer**: 1024 units with stateful processing to retain context across batches.\n",
        "- **Dense Layer**: Outputs logits for each character in the vocabulary.\n",
        "- **Model Summary**: Shows the architecture and parameter count (~4 million).\n"
      ],
      "metadata": {
        "id": "2O7lQHWY_RlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Train the Model**"
      ],
      "metadata": {
        "id": "Ip9dYqlg_ZjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for more epochs\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    epochs=30  # Increase from 10 to 30\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0H_Bjkq_G_B",
        "outputId": "5c22a478-4c18-4268-87c7-65816802a66c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 86ms/step - loss: 3.1745\n",
            "Epoch 2/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 78ms/step - loss: 2.0972\n",
            "Epoch 3/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - loss: 1.8141\n",
            "Epoch 4/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 74ms/step - loss: 1.6460\n",
            "Epoch 5/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - loss: 1.5410\n",
            "Epoch 6/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - loss: 1.4712\n",
            "Epoch 7/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - loss: 1.4195\n",
            "Epoch 8/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 75ms/step - loss: 1.3765\n",
            "Epoch 9/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - loss: 1.3458\n",
            "Epoch 10/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - loss: 1.3131\n",
            "Epoch 11/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - loss: 1.2893\n",
            "Epoch 12/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - loss: 1.2639\n",
            "Epoch 13/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - loss: 1.2387\n",
            "Epoch 14/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - loss: 1.2210\n",
            "Epoch 15/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - loss: 1.1941\n",
            "Epoch 16/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 76ms/step - loss: 1.1781\n",
            "Epoch 17/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - loss: 1.1530\n",
            "Epoch 18/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - loss: 1.1314\n",
            "Epoch 19/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - loss: 1.1084\n",
            "Epoch 20/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 1.0853\n",
            "Epoch 21/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 78ms/step - loss: 1.0610\n",
            "Epoch 22/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - loss: 1.0357\n",
            "Epoch 23/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - loss: 1.0131\n",
            "Epoch 24/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - loss: 0.9842\n",
            "Epoch 25/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - loss: 0.9557\n",
            "Epoch 26/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - loss: 0.9298\n",
            "Epoch 27/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - loss: 0.9036\n",
            "Epoch 28/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - loss: 0.8718\n",
            "Epoch 29/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - loss: 0.8453\n",
            "Epoch 30/30\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 76ms/step - loss: 0.8151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Text Generation**"
      ],
      "metadata": {
        "id": "d3gd1SM1_dRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Generate Text Function**"
      ],
      "metadata": {
        "id": "Z0XaFalL_iFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_generate=1000):\n",
        "    input_eval = [char2idx[char] for char in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "    temperature = 0.7  # Lower temperature for less randomness\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "mPmkhpnR_HcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- **Temperature**: Controls the randomness of predictions. Lower values make the output more deterministic.\n",
        "- The model predicts the next character iteratively, using its previous output as input.\n"
      ],
      "metadata": {
        "id": "biRZymv__kGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Generate Sample Text**\n"
      ],
      "metadata": {
        "id": "PEfasbIS_mpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text starting with \"ROMEO\"\n",
        "print(generate_text(model, start_string=\"ROMEO\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixQgBJ1c_kbJ",
        "outputId": "1ceb1bb8-ccf0-4500-f66c-08e770958fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "\n",
            "\n",
            "This, dweande is angandes y tht t thou, ma be thand thator me thes peer wer t yorsome, f redor:\n",
            "Princou omy, t ma thas fof,\n",
            "Sears oound the I myour yourariny bend, thisthe spodeeamy tur ath t ticat ananouspamat pr ghe t an y ll tout ape, an h in f isend ther g ant me youror ar the g t ghoouth winomournouthyond hars hathasit by he te\n",
            "ARALI'O:\n",
            "I oind heree f ofustous t wee thareseser nde tute he t sesowind anoter he matlfors thapenorst t st were br fare bl s thest tr hes I lfoure catit y.\n",
            "TE matht INCHAn yout thes ben her m tucethinoous cand masiouger st theno thouthe; y, hatr amyotheweat by se! bor t thand t br s wet he acat iosiome thesed trakeatha athare mo hanound thout ang heshakee th! thar s!\n",
            "My to ber tos t mas,\n",
            "AUnondecoof s s t, INAS:\n",
            "HEO:\n",
            "Torerely the thicuickin t s y willous our g ge\n",
            "Than f ake l ved t han yon t, h thous we iscke irdatiend wilindw t t, wat bre g'swnd pou ang'\n",
            "INE:\n",
            "Tis ayoreat co hat fousthal ingerO:\n",
            "OUSPHe t bl d le, lar he ind mome d t t, s nther l turere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretation**\n",
        "- The generated text mimics Shakespearean language and structure.\n",
        "- The model learns context (e.g., character names, poetic formatting).\n"
      ],
      "metadata": {
        "id": "7d4OSJZ8_rW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Advantages and Disadvantages**\n",
        "\n",
        "### **Advantages**\n",
        "- **Sequential Data Handling**: RNNs excel at processing sequences (text, time series).\n",
        "- **Context Retention**: LSTMs capture long-term dependencies in text.\n",
        "- **Creative Output**: Can generate novel text that mimics the training data.\n",
        "\n",
        "### **Disadvantages**\n",
        "- **Computational Cost**: Training LSTMs is resource-intensive.\n",
        "- **Vanishing Gradients**: Basic RNNs struggle with long sequences (mitigated by LSTMs).\n",
        "- **Overfitting**: Can memorize training data if not regularized.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Conclusion**\n",
        "\n",
        "### **Key Learnings**\n",
        "- RNNs process sequential data by maintaining a hidden state.\n",
        "- LSTMs address the vanishing gradient problem with gated mechanisms.\n",
        "- Text generation involves iterative prediction of the next character.\n",
        "\n",
        "### **Next Steps**\n",
        "- Experiment with **GRUs** (Gated Recurrent Units) for efficiency.\n",
        "- Use **Transformer models** (e.g., GPT-2) for more coherent long-form text.\n",
        "- Fine-tune hyperparameters (sequence length, temperature) for better results.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. References**\n",
        "- TensorFlow RNN Guide: [Text Generation with RNNs](https://www.tensorflow.org/text/tutorials/text_generation)\n",
        "- Hochreiter & Schmidhuber (1997): [LSTM Paper](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
        "- Shakespeare Dataset: [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/shakespeare)\n"
      ],
      "metadata": {
        "id": "g_O-dPlB_uME"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5nE3ZiBDr-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RlnxObzuDsBi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}