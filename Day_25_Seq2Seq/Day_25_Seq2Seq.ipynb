{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ultimate Guide to Sequence-to-Sequence (Seq2Seq) Models for Translation**"
      ],
      "metadata": {
        "id": "_qWMDywwmgAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction**\n",
        "\n",
        "### **What is a Seq2Seq Model?**\n",
        "A **Sequence-to-Sequence (Seq2Seq)** model is a neural network architecture designed for tasks where the input and output are sequences of varying lengths. It consists of two main components:\n",
        "1. **Encoder**: Processes the input sequence and encodes it into a fixed-size context vector.\n",
        "2. **Decoder**: Generates the output sequence step-by-step, using the context vector as input.\n",
        "\n",
        "### **Applications**\n",
        "- **Machine Translation**: Translating text from one language to another (e.g., English to French).\n",
        "- **Text Summarization**: Generating concise summaries of long documents.\n",
        "- **Chatbots**: Generating conversational responses.\n",
        "\n",
        "### **Objective**\n",
        "To build and train a Seq2Seq model for **English-to-French translation** using TensorFlow/Keras.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Metadata and Dataset Overview**\n",
        "\n",
        "### **Dataset Used**\n",
        "- **Dataset Name**: English-French Translation Dataset\n",
        "- **Source**: [Tatoeba Project](https://tatoeba.org/)\n",
        "- **Description**: A collection of sentence pairs in English and French.\n",
        "\n",
        "### **Acknowledgement**\n",
        "This dataset is publicly available and widely used for educational purposes in NLP.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tm-T_ojGmjpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Loading and Exploring the Dataset**"
      ],
      "metadata": {
        "id": "FdkZwStFmlYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Load the Dataset**"
      ],
      "metadata": {
        "id": "qKEKcNDImpaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "color_pal = sns.color_palette()\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Download the dataset\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'fra-eng.zip',\n",
        "    origin='http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip',\n",
        "    extract=False  # Do not extract automatically\n",
        ")\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(os.path.dirname(path_to_zip))\n",
        "\n",
        "# Path to the extracted file\n",
        "path_to_file = os.path.join(os.path.dirname(path_to_zip), 'fra.txt')\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(path_to_file):\n",
        "    raise FileNotFoundError(f\"The file was not found at {path_to_file}\")\n",
        "\n",
        "# Load the dataset\n",
        "with open(path_to_file, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "# Preview the dataset\n",
        "print(f\"Total sentence pairs: {len(lines)}\")\n",
        "print(lines[:5])  # Print the first 5 sentence pairs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb_ukyZsmgsV",
        "outputId": "e0135f51-c5e4-40ee-9ad6-5de197df9f57"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentence pairs: 167131\n",
            "['Go.\\tVa !', 'Hi.\\tSalut !', 'Run!\\tCours\\u202f!', 'Run!\\tCourez\\u202f!', 'Who?\\tQui ?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- The dataset is downloaded and extracted using TensorFlow utilities.\n",
        "- Each line contains an English sentence and its corresponding French translation, separated by a tab (`\\t`).\n"
      ],
      "metadata": {
        "id": "qSJwlqTLnG02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Preprocess the Dataset**"
      ],
      "metadata": {
        "id": "S9j-BxeTxcHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Add <start> and <end> tokens to the target sentences\n",
        "def preprocess_sentence(sentence, is_target=False):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)  # Add spaces around punctuation\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # Remove extra spaces\n",
        "    sentence = sentence.strip()\n",
        "    if is_target:\n",
        "        sentence = '<start> ' + sentence + ' <end>'  # Add <start> and <end> tokens\n",
        "    return sentence\n",
        "\n",
        "# Preprocess the dataset\n",
        "word_pairs = [[preprocess_sentence(w[0]), preprocess_sentence(w[1], is_target=True)] for w in [l.split('\\t') for l in lines[:10000]]]\n",
        "print(word_pairs[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDHgz3Hlms46",
        "outputId": "2ea544b9-2b73-432e-8fbb-08be255b0372"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['go .', '<start> va ! <end>'], ['hi .', '<start> salut ! <end>'], ['run !', '<start> cours\\u202f ! <end>'], ['run !', '<start> courez\\u202f ! <end>'], ['who ?', '<start> qui ? <end>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- Each sentence is preprocessed by:\n",
        "  - Converting to lowercase.\n",
        "  - Adding spaces around punctuation.\n",
        "  - Removing extra spaces.\n",
        "- The dataset is limited to the first 10,000 pairs for faster training.\n",
        "\n"
      ],
      "metadata": {
        "id": "wgol1yEHnPcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Tokenization and Padding**\n",
        "\n",
        "### **Code: Tokenize the Sentences**"
      ],
      "metadata": {
        "id": "6xkvnwN7nTKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize English and French sentences\n",
        "def tokenize(lang):\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(lang)\n",
        "    tensor = tokenizer.texts_to_sequences(lang)\n",
        "    tensor = pad_sequences(tensor, padding='post')\n",
        "    return tensor, tokenizer\n",
        "\n",
        "# Split into input (English) and target (French) sequences\n",
        "input_tensor, input_tokenizer = tokenize([pair[0] for pair in word_pairs])\n",
        "target_tensor, target_tokenizer = tokenize([pair[1] for pair in word_pairs])\n",
        "\n",
        "# Vocabulary sizes\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"Input vocabulary size: {input_vocab_size}\")\n",
        "print(f\"Target vocabulary size: {target_vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6cYlStAnPqx",
        "outputId": "2221f6b5-5a04-4a56-bcb1-3b654f815c90"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vocabulary size: 2146\n",
            "Target vocabulary size: 4859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- **Tokenizer**: Converts sentences into sequences of integers.\n",
        "- **Padding**: Ensures all sequences have the same length by adding zeros at the end.\n",
        "- **Vocabulary Size**: The number of unique words in each language.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Build the Seq2Seq Model**\n"
      ],
      "metadata": {
        "id": "Vngc_O81nVux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Concepts**\n",
        "1. **Encoder**:\n",
        "   - Processes the input sequence (English) and encodes it into a context vector.\n",
        "   - Uses an LSTM or GRU layer.\n",
        "2. **Decoder**:\n",
        "   - Generates the output sequence (French) step-by-step.\n",
        "   - Uses an LSTM or GRU layer with attention (optional).\n",
        "\n"
      ],
      "metadata": {
        "id": "esvkOod-nYBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Define the Encoder**"
      ],
      "metadata": {
        "id": "r1S8XJdvnatJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]  # Context vector\n"
      ],
      "metadata": {
        "id": "FC2Se8ZqnWFQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- **Embedding Layer**: Converts word indices into dense vectors.\n",
        "- **LSTM Layer**: Processes the sequence and outputs the final hidden states (`state_h`, `state_c`), which serve as the context vector.\n"
      ],
      "metadata": {
        "id": "MlJW-FDpncHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Define the Decoder**"
      ],
      "metadata": {
        "id": "xDmuX1zknho3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(target_vocab_size, 256)(decoder_inputs)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n"
      ],
      "metadata": {
        "id": "X2e0smi3ncU4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- **Embedding Layer**: Converts French word indices into dense vectors.\n",
        "- **LSTM Layer**: Uses the context vector (`encoder_states`) as its initial state.\n",
        "- **Dense Layer**: Outputs probabilities for each word in the French vocabulary.\n"
      ],
      "metadata": {
        "id": "urHc9qPPnjSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Build the Model**"
      ],
      "metadata": {
        "id": "UBZm2KmJnltn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Combine encoder and decoder\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "3R8rp49Xnjdc",
        "outputId": "4f27ce2b-2817-4e51-82bf-170e33d06a76"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_5             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m549,376\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m1,243,904\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m525,312\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                           │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),    │        \u001b[38;5;34m525,312\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],          │\n",
              "│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4859\u001b[0m)     │      \u001b[38;5;34m1,248,763\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_5             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">549,376</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,243,904</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],          │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4859</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,248,763</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,092,667\u001b[0m (15.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,092,667</span> (15.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,092,667\u001b[0m (15.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,092,667</span> (15.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- The model takes two inputs: English sentences (`encoder_inputs`) and French sentences (`decoder_inputs`).\n",
        "- It outputs the predicted French sentence (`decoder_outputs`).\n"
      ],
      "metadata": {
        "id": "QrcOUapYno83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Train the Model**\n",
        "\n",
        "### **Code: Prepare the Data**\n"
      ],
      "metadata": {
        "id": "lF0XGO_5nqVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_input, val_input, train_target, val_target = train_test_split(\n",
        "    input_tensor, target_tensor, test_size=0.2\n",
        ")\n",
        "\n",
        "# Prepare decoder input and output\n",
        "train_decoder_input = train_target[:, :-1]  # Exclude the last token\n",
        "train_decoder_output = train_target[:, 1:]  # Exclude the first token\n",
        "\n",
        "val_decoder_input = val_target[:, :-1]\n",
        "val_decoder_output = val_target[:, 1:]\n"
      ],
      "metadata": {
        "id": "waZt0tcGnoet"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- The decoder input is shifted by one token to predict the next word in the sequence.\n"
      ],
      "metadata": {
        "id": "3N0eXwn6nuXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Train the Model**"
      ],
      "metadata": {
        "id": "DLAq740Znvp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [train_input, train_decoder_input],\n",
        "    train_decoder_output,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([val_input, val_decoder_input], val_decoder_output)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROPqSSb4nohH",
        "outputId": "ce4e224c-8866-4cd4-f59b-3f53496d839b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.5594 - loss: 4.0934 - val_accuracy: 0.7228 - val_loss: 1.9643\n",
            "Epoch 2/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.7264 - loss: 1.8531 - val_accuracy: 0.7300 - val_loss: 1.7788\n",
            "Epoch 3/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.7327 - loss: 1.6750 - val_accuracy: 0.7389 - val_loss: 1.6737\n",
            "Epoch 4/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.7435 - loss: 1.5532 - val_accuracy: 0.7571 - val_loss: 1.5820\n",
            "Epoch 5/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.7661 - loss: 1.4405 - val_accuracy: 0.7734 - val_loss: 1.5038\n",
            "Epoch 6/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7805 - loss: 1.3355 - val_accuracy: 0.7817 - val_loss: 1.4384\n",
            "Epoch 7/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.7924 - loss: 1.2485 - val_accuracy: 0.7920 - val_loss: 1.3828\n",
            "Epoch 8/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8018 - loss: 1.1560 - val_accuracy: 0.7999 - val_loss: 1.3266\n",
            "Epoch 9/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8104 - loss: 1.0681 - val_accuracy: 0.8037 - val_loss: 1.2870\n",
            "Epoch 10/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.8161 - loss: 1.0009 - val_accuracy: 0.8076 - val_loss: 1.2574\n",
            "Epoch 11/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.8240 - loss: 0.9277 - val_accuracy: 0.8161 - val_loss: 1.2248\n",
            "Epoch 12/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.8314 - loss: 0.8647 - val_accuracy: 0.8178 - val_loss: 1.2033\n",
            "Epoch 13/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.8367 - loss: 0.8124 - val_accuracy: 0.8204 - val_loss: 1.1836\n",
            "Epoch 14/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8431 - loss: 0.7593 - val_accuracy: 0.8240 - val_loss: 1.1664\n",
            "Epoch 15/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.8498 - loss: 0.7116 - val_accuracy: 0.8262 - val_loss: 1.1511\n",
            "Epoch 16/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.8564 - loss: 0.6618 - val_accuracy: 0.8305 - val_loss: 1.1346\n",
            "Epoch 17/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.8643 - loss: 0.6140 - val_accuracy: 0.8309 - val_loss: 1.1262\n",
            "Epoch 18/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8696 - loss: 0.5789 - val_accuracy: 0.8325 - val_loss: 1.1162\n",
            "Epoch 19/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.8764 - loss: 0.5415 - val_accuracy: 0.8353 - val_loss: 1.1038\n",
            "Epoch 20/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.8823 - loss: 0.5004 - val_accuracy: 0.8372 - val_loss: 1.0983\n",
            "Epoch 21/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8887 - loss: 0.4688 - val_accuracy: 0.8376 - val_loss: 1.0952\n",
            "Epoch 22/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.8961 - loss: 0.4341 - val_accuracy: 0.8400 - val_loss: 1.0870\n",
            "Epoch 23/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9017 - loss: 0.4028 - val_accuracy: 0.8408 - val_loss: 1.0851\n",
            "Epoch 24/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9074 - loss: 0.3751 - val_accuracy: 0.8425 - val_loss: 1.0798\n",
            "Epoch 25/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9123 - loss: 0.3496 - val_accuracy: 0.8427 - val_loss: 1.0794\n",
            "Epoch 26/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9175 - loss: 0.3276 - val_accuracy: 0.8433 - val_loss: 1.0773\n",
            "Epoch 27/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9212 - loss: 0.3054 - val_accuracy: 0.8445 - val_loss: 1.0768\n",
            "Epoch 28/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9269 - loss: 0.2808 - val_accuracy: 0.8455 - val_loss: 1.0758\n",
            "Epoch 29/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.9306 - loss: 0.2647 - val_accuracy: 0.8465 - val_loss: 1.0793\n",
            "Epoch 30/30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9345 - loss: 0.2447 - val_accuracy: 0.8477 - val_loss: 1.0758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- The model is trained for 30 epochs with a batch size of 64.\n",
        "- Validation data is used to monitor performance.\n"
      ],
      "metadata": {
        "id": "TUVbcVCQnzG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **7. Inference (Translation)**\n"
      ],
      "metadata": {
        "id": "TYyVybtjn1Eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Build Inference Models**"
      ],
      "metadata": {
        "id": "y6zuRsbxn2aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder inference model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_embedding, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n"
      ],
      "metadata": {
        "id": "jPZ57w2snojq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- The inference models are used to generate translations step-by-step.\n"
      ],
      "metadata": {
        "id": "x-sn5fnqn5gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code: Translate Function**"
      ],
      "metadata": {
        "id": "81hRs8W5n8Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(input_seq):\n",
        "    # Encode the input sequence\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Initialize the decoder input with the <start> token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
        "\n",
        "    # Generate the translation\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample the next word\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = target_tokenizer.index_word[sampled_token_index]\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Exit condition\n",
        "        if sampled_word == '<end>' or len(decoded_sentence) > 50:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence and states\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()"
      ],
      "metadata": {
        "id": "V8cYmzqFnomA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- The function translates an English sentence into French using the trained model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z4Rt_ONMn-n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Results and Evaluation**\n",
        "\n",
        "### **Code: Test Translation**\n"
      ],
      "metadata": {
        "id": "bV7BJKWLoAX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test translation\n",
        "input_seq = input_tensor[0:1]  # First English sentence\n",
        "translated_sentence = translate(input_seq)\n",
        "print(f\"Input: {word_pairs[0][0]}\")\n",
        "print(f\"Translation: {translated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek9uuqRnnonh",
        "outputId": "11543d63-4673-45c7-bbc6-f617a3fd8f6e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Input: go .\n",
            "Translation: va ! <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Conclusion**\n",
        "\n",
        "### **Key Learnings**\n",
        "- Seq2Seq models are powerful for sequence-based tasks like translation.\n",
        "- The encoder-decoder architecture effectively handles variable-length sequences.\n",
        "- Attention mechanisms (not covered here) can further improve performance.\n",
        "\n",
        "### **Next Steps**\n",
        "- Add **attention mechanisms** to improve translation quality.\n",
        "- Experiment with **larger datasets** and **deeper models**.\n",
        "- Use **beam search** for better decoding.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. References**\n",
        "- TensorFlow Seq2Seq Tutorial: [Neural Machine Translation](https://www.tensorflow.org/tutorials/text/nmt_with_attention)\n",
        "- Tatoeba Dataset: [Tatoeba Project](https://tatoeba.org/)\n",
        "- Seq2Seq Paper: [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "\n"
      ],
      "metadata": {
        "id": "dTYvlp_doGER"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31WHAL9hoHt-"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}